import cv2, os
import numpy as np
import pickle
import copy

from scipy.stats import norm
from math import sqrt

import matplotlib.pyplot as plt

import numpy as np
from PIL import Image
from cv_bridge import CvBridge
# ros
import rospy
import sensor_msgs.msg
import rospkg

from ariadne_plus.srv import getSplines, getSplinesRequest, getSplinesResponse
from scipy.interpolate import splev, splrep, splprep

import sys
sys.path.append('../../')
from utils.vision.rgb_camera import image_converter

def generateImage(img_np):
    img = Image.fromarray(img_np).convert("RGB") 
    msg = sensor_msgs.msg.Image()
    msg.header.stamp = rospy.Time.now()
    msg.height = img.height
    msg.width = img.width
    msg.encoding = "rgb8"
    msg.is_bigendian = False
    msg.step = 3 * img.width
    msg.data = np.array(img).tobytes()
    return msg

def apply_crop(img, crop_corners):
    ## extract feature_map from img by using the 2d bounding box
    height = img.shape[0]
    width = img.shape[1]
    x1 = crop_corners[0,0]
    x2 = crop_corners[1,0]
    y1 = crop_corners[0,1]
    y2 = crop_corners[2,1]
    feature_map = []
    cropped_image = np.zeros((y2-y1, x2-x1, 3), dtype=np.uint8)
    for iy in range(height):
        for ix in range(width):
            j = cv2.pointPolygonTest(crop_corners, (ix,iy), False)
            if j > 0:
                cropped_image[iy-y1, ix-x1] = img[iy, ix]

    return cropped_image

## overlay the mask generated by DeepLabV3 to the image
def draw_dl_mask(img, crop_corners, mask):
    x1 = crop_corners[0,0]
    x2 = crop_corners[1,0]
    y1 = crop_corners[0,1]
    y2 = crop_corners[2,1]
    output = copy.deepcopy(img)
    resized_mask = cv2.resize(mask, (x2-x1,y2-y1))
    for iy in range(y2-y1):
        for ix in range(x2-x1):
            if resized_mask[iy,ix,0] > 100:
                rgb = output[iy+y1, ix+x1]
                if rgb[1] + 100 > 255:
                    rgb[1] == 255
                else:
                    rgb[1] += 100

                rgb[0] = rgb[0]/2
                rgb[2] = rgb[2]/2

    return output

## draw splines on the image
def draw_spline(img, spline, type):
    output = copy.deepcopy(img)
    for i in range(len(spline[0])):
        x = int(spline[0][i])
        y = int(spline[1][i])
        output = cv2.circle(output, (x,y), radius=1, color=(255, 0, 0), thickness=-1)

    return output

## draw grasping point, given a point
def draw_gp(img, pt):
    output = copy.deepcopy(img)

    return output

def eval_spline(tck, crop_corners, nn_size):
    x1 = crop_corners[0,0]
    x2 = crop_corners[1,0]
    y1 = crop_corners[0,1]
    y2 = crop_corners[2,1]

    t = np.array(tck.t)
    c = np.array([tck.cx,tck.cy])
    k = int(tck.k)
    tck = [t,c,k]
    spline = splev(np.linspace(0,1,200), tck)

    x = [i*(x2-x1)/nn_size[0]+x1 for i in spline[0]]
    y = [i*(y2-y1)/nn_size[1]+y1 for i in spline[1]]
    return [x,y]

def check_spline(spline, box2d, l):
    ## No need to sort, I think
    on_rod = False
    cnt = 0
    while cnt < len(spline[1]):
        j = cv2.pointPolygonTest(box2d, (spline[0][cnt],spline[1][cnt]), False)
        if j > 0:
            ## on rod detected
            on_rod = True
            break

        cnt += 1

    if on_rod:
        while cnt < len(spline[1]):
            j = cv2.pointPolygonTest(box2d, (spline[0][cnt],spline[1][cnt]), False)
            if j > 0:
                cnt += 1
            else:
                ## starting from the 
                break

        acc_pixel_l = 0
        while cnt < len(spline[1])-1:
            acc_pixel_l += sqrt((spline[0][cnt]-spline[0][cnt+1])**2+(spline[1][cnt]-spline[1][cnt+1])**2)
            if acc_pixel_l > l:
                return (int(spline[0][cnt]), int(spline[1][cnt]))

            cnt += 1

        return None

    else:
        ## spline not starts from the rod, skip
        return None

class rope_detect:
    def __init__(self, rod_info):
        self.pub = rospy.Publisher('grasping_point_detect', sensor_msgs.msg.Image, queue_size=10)
        self.rod_info = rod_info
        self.mask = None
        self.masked_img = None
        
        ## box2d:
        ## 3----4
        ## |    |
        ## 2----1
        sort1 = rod_info.box2d[rod_info.box2d[:,1].argsort()]
        ## upper and lower boundry
        y1 = sort1[0,1]-10
        # y1 = sort1[-2,1]-10
        # y2 = img.shape[0]
        ## image size is 1280*720
        y2 = 720
        sort2 = rod_info.box2d[rod_info.box2d[:,0].argsort()]
        ## left and right boundry
        x1 = sort2[0,0]-10
        x2 = sort2[-1,0]+10

        if (x2-x1)/(y2-y1) > 640/480:
            ## too rectangle
            y1 = int(y2-(x2-x1)*480/640)
        else:
            ## too square
            xc = (x2+x1)/2
            width = (y2-y1)*640/480
            x1 = int(xc-width/2)
            x2 = int(xc+width/2)

        self.crop_corners= np.array([[x1,y1],[x2,y1],[x2,y2],[x1,y2]])

        self.bridge = CvBridge()

        x3_p = rod_info.box2d[2][0]
        x4_p = rod_info.box2d[3][0]
        y3_p = rod_info.box2d[2][1]
        y4_p = rod_info.box2d[3][1]
        l_pixel = sqrt((x3_p-x4_p)**2+(y3_p-y4_p)**2)
        self.scale = rod_info.l/l_pixel

    def gp_estimation(self, img, l=0.1, plt_debug=False):
        ## estimating grasping point.
        ## Input: given an image and expecting length (l) of the rope (from rod to the grasping point)

        ## crop to get the workspace
        cropped = apply_crop(img, self.crop_corners)

        input_img = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)
        input_img = cv2.resize(input_img, (640,480)) # resize necessary for the network model
        img_msg = generateImage(input_img)

        rospy.wait_for_service('get_splines')
        try:
            get_cable = rospy.ServiceProxy('get_splines', getSplines)
            req = getSplinesRequest()
            req.input_image = img_msg
            resp1 = get_cable(req)

        except rospy.ServiceException as e:
            print("Service call failed: %s"%e)

        # print("get cable:")
        # print(resp1.tck)
        dl_mask = self.bridge.imgmsg_to_cv2(resp1.mask_image, desired_encoding='passthrough')

        self.masked_img = draw_dl_mask(img, self.crop_corners, dl_mask)

        ## img.shape[1]: x/width
        ## img.shape[0]: y/height

        ## Assume the measure of pixels and actual objects are uniformly scaled.
        
        # print("phsical to pixel scale is: %f/%f=%f"%(rod_info.l, l_pixel, self.scale))

        splines = []
        for i in range(len(resp1.tck)):
            splines.append(eval_spline(resp1.tck[i], self.crop_corners, (640,480)))
            self.masked_img = draw_spline(self.masked_img, splines[-1], 'selected')

        if plt_debug:
            ## show by matplotlib, original image with mask overlaid, mask image, detected splines
            fig = plt.figure(figsize=(12,10))
            ax0 = plt.subplot2grid((2,2),(0,0))
            ax1 = plt.subplot2grid((2,2),(0,1))
            ax2 = plt.subplot2grid((2,2),(1,0),colspan=2)

            ax0.imshow(cv2.cvtColor(self.masked_img, cv2.COLOR_BGR2RGB))
            ax1.imshow(cv2.cvtColor(dl_mask, cv2.COLOR_BGR2RGB))

            ax2.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            for s in splines:
                ax2.plot(s[0], s[1], color='c', linewidth=2)

            plt.tight_layout()
            plt.show()

        grasping_point = None
        for s in splines:
            grasping_point = check_spline(s, self.rod_info.box2d, l/self.scale)
            if grasping_point is None:
                continue
            else:
                break

        if grasping_point is None:
            print("No possible rope end is found")
            self.pub.publish(self.bridge.cv2_to_imgmsg(self.masked_img, encoding='passthrough'))
            return None
        else:
            ## return estimated grasping point position
            # center of the rectangle, in pixel
            xc_p = (self.rod_info.box2d[2][0] + self.rod_info.box2d[0][0])/2
            yc_p = (self.rod_info.box2d[2][1] + self.rod_info.box2d[0][1])/2

            dx_p = grasping_point[0] - xc_p
            dy_p = grasping_point[1] - yc_p

            self.masked_img = cv2.circle(self.masked_img, (int(grasping_point[0]),int(grasping_point[1])), radius=5, color=(0, 0, 255), thickness=-1)
            self.pub.publish(self.bridge.cv2_to_imgmsg(self.masked_img, encoding='passthrough'))

            # estimate distance, actual, measured in meters
            dy = dx_p * self.scale
            dz = -dy_p * self.scale

            x = self.rod_info.pose.position.x + self.rod_info.r
            y = self.rod_info.pose.position.y + dy
            z = self.rod_info.pose.position.z + dz
            print("found grasping point: %.3f, %.3f, %.3f"%(x, y, z))
            return [x, y, z]


if __name__ == '__main__': 
    with open('rod_info.pickle', 'rb') as handle:
        rod_info = pickle.load(handle)

    # img = cv2.imread('image1.jpg')
    ic = image_converter()
    rospy.init_node('ariadne_test', anonymous=True)
    rospy.sleep(1)
    while ic.has_data==False:
            print('waiting for RGB data')
            rospy.sleep(0.1)

    serial_number = 0
    rd = rope_detect(rod_info)
    rd.gp_estimation(ic.cv_image, 0.1)
    # for i in range(30):
    #     main(ic.cv_image, rod_info.box2d, 0.1)
    #     input("test: "+str(i))